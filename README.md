# Debias-GAN
Natural Language Processing (NLP) machine learning models are gaining popularity in various context such as resume screening, college admission, emotion assessment, repeated crime prediction, and more. Consequently, it becomes increasingly important to recognize the role they play in contributing to societal biases and stereotypes. As the majority NLP models are trained on historic data, while achieving high performance on many language-understanding tasks, these models often lack optimization for reducing implicit biases, and in some cases, promote those biases. The bias in machine learning model refers to a situation where the algorithms express strong association amongst attributes that should not be correlated in reality.  In this white paper, we propose a general framework, debias-GAN, to address this issue by explicitly augment training dataset for NLP models with underrepresented instances synthesized by a pretrained sequence generating model. As proof-of-concept, we chose to decorrelate user ethnicity and tweets being conversational in a deep classification model. The synthetic data is generated by a targeted language model (LM) that generates realistic but user ethnicity oblivious tweets. We trained such debiased LM with generative adversarial networks (GAN) through reinforcement learning (RL) by penalizing sequences with strong indication of user ethnicity via policy update. The reward is provided by an independently trained classifier that identifies user ethnicity from tweets. We experimented with the ratio of mixed dataset and tested the debiasing impact using three fairness metrics. The debias-GAN is able to improve the fairness metrics of the classifier to up to 7 times while maintaining classification performance.

##  Literature
This is the code used in the paper that lays the foundation of the generator policy :

**"SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient"**
Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu. arxiv preprint 2017. [Paper](https://arxiv.org/abs/1609.05473)

These are the the papers that inspires the debiasing method used in our model:
**"Adversarial Removal of Demographic Attributes from Text Data"**
Yanai Elazar, Yoav Goldberg. arxiv preprint 2018. [Paper](https://arxiv.org/abs/1808.06640)

**"Mitigating Political Bias in Language Models Through Reinforced Calibration"**
Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, Lili Wang, Soroush Vosoughi. arxiv preprint 2021. [Paper](https://arxiv.org/abs/2104.14795)


### Prerequisites
* python 2.7
* tensorflow 1
* GPU Tesla K80 (multiple nodes, at least 5)

```sh
# create a docker container
docker run -d -it -p 8888:8888 --gpus all -v /home/ubuntu/bias_in_ai:/seqGANtf tensorflow/tensorflow:1.0.0-rc1-gpu
```
```sh
# get into the container
docker exec -it $dockername$ bash
```
```sh
```

### Running the models:
* baseline_mention_classifier.py: training the mention classifier on real tweets for baseline fairness metrics calculation and classification performance
* pretrain_ethnicity_classifier.py: training the ethnicity classifier on real tweets, and this pretrained model will be used in debias-GAN to create ethnicity oblivious tweets
* pretrain_g.py: pretrain generator using supervised learning and MLE
* pretrain_d.py: pretrain discriminator using a mix of real and synthetic tweets before GAN
* tweets_gan_vanilla.py: training the generator policy in adversarial network as well as supervised learning before debiasing
* tweets_gan_with_classifier.py: training the generator with debiasing in adversarial network. Compared to tweets_gan_vanilla, this script incorporate ethnicity_classifier to provide additional reward to the generator policy to guide it to generate ethnicity oblivious yet realistic tweets
* preproc_10_token_seq_for_mention_classifier.py: using the trained generator, generate ethnicity oblivious tweets, preprocess, and mix with real tweets for train a new mention classifier
* debiased_mention_classifier.py: training the mention classifier using mixed data to mitigate ethnicity bias

Each script has its parameters to run with. Baseline_mention_classifier.py, pretrain_ethnicity_classifier.py, and pretrain_g.py can be ran in parallel on different gpu nodes. After pretrain_g.py finishes, run pretrain_d.py. Once pretrain_d.py is completed, run tweets_gan_vanilla.py. After all aforementioned scripts are completed, run tweets_gan_with_classifier.py. Lastly, run preproc_10_token_seq_for_mention_classifier.py before debiased_mention_classifier.py.

### Supporting modules:
* classifier.py: classifier with CNN architecture, used in ethnicity classifier and tweets_gan_with_classifier.py
* mention_classifier_node_4.py: same CNN architecture as classifier.py, but assigned to GPU node 4
* generator.py: generator policy with LSTM artchitecture, used in pretrain_g.py
* restore_generator.py: load generator pretrained weights, used in tweets_gan_vanilla.py
* restore_generator_node_2.py: load generator pretrained wegihts, same as restore_generator.py, but assigned to GPU node 2, used in tweets_gan_with_classifier.py
* discriminator.py: discriminator with CNN architecture, used in pretrain_d.py and tweets_gan_vanilla.py
* discriminator_node_3.py: same as discriminator, but assigned to GPU node 3, used in tweets_gan_with_classifier.py
* data_loader.py: load data in batches for all models
* rollout.py: MC rollout policy for generator policy in adversarial training. This script does not include classifier reward, is used in tweets_gan_vanilla.py
* rollout_with_classifier.py: same as rollout.py but added classifier reward


### Preprocess raw datasets
The raw data is converted into sequence using [BERTweet](https://github.com/VinAIResearch/BERTweet) tokenization using embedding.py, which runs in a separate instance.

### Preprocessing Prerequisites

### <a name="install2"></a> Installation

 -  Python 3.6+, and PyTorch 1.1.0+ (or TensorFlow 2.0+)
 -  Install `transformers`:
    - `git clone https://github.com/huggingface/transformers.git`
    - `cd transformers`
    - `pip3 install --upgrade .`
 - Install `emoji`: `pip3 install emoji`

### <a name="models2"></a> Pre-trained models


Model | #params | Arch. | Pre-training data
---|---|---|---
`vinai/bertweet-base` | 135M | base | 845M English Tweets (cased)
`vinai/bertweet-covid19-base-cased` | 135M | base | 23M COVID-19 English Tweets (cased)
`vinai/bertweet-covid19-base-uncased` | 135M | base | 23M COVID-19 English Tweets (uncased)

### Additional codes
* weibo_embedding.py: preprocessing weibo (Chinese language)
